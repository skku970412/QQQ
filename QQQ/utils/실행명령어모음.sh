CUDA_VISIBLE_DEVICES=3,4,5 python3 examples/quant_model.py   --model_path     ./models/Llama-3.1-8B   --tokenizer_path ./models/Llama-3.1-8B   --dtype float16   --smooth false   --rotation true   --dataset wikitext2   --nsamples 128   --w_quantizer FixedQuantize   --w_group_size -1   --gptq_mse true   --gptq_groupsize -1   --save_path ./qqq-llama3-8b_test_more_samples   --batch_size 1


grep -R --include="*.py" "tokenizer(" ./QQQ -n
len(tokenizer)
grep -R --include="*.py" "_update_causal_mask" ./QQQ -n


# activation per-channel quant + weight per-group quant + groupsize 128

CUDA_VISIBLE_DEVICES=1,2,5 python3 examples/quant_model.py   --model_path     ./models/Llama-3.1-8B   --tokenizer_path ./models/Llama-3.1-8B   --dtype float16   --smooth false   --rotation true   --dataset wikitext2   --nsamples 128   --w_quantizer GroupFixedQuantize   --w_group_size 128   --gptq_mse true   --gptq_roupsize 128   --save_path ./qqq-llama3-8b_g128   --batch_size 8g

# llama3.1 8b ÌÄÄÌä∏Ìä∏
CUDA_VISIBLE_DEVICES=2,3,5 python3 examples/quant_model.py   --model_path     ./models/Llama-3.1-8B   --tokenizer_path ./models/Llama-3.1-8B   --dtype float16   --smooth false   --rotation false   --dataset wikitext2   --nsamples 128   --w_quantizer FixedQuantize   --w_group_size -1   --gptq_mse true   --gptq_groupsize -1   --save_path ./qqq-llama3-8b_sdpa --batch_size 1


# llama3.1 8b base perchannel
PYTHONPATH=. CUDA_VISIBLE_DEVICES=2,3,5 python3 examples/quant_model.py   --model_path     ./models/Llama-3.1-8B     --tokenizer_path ./models/Llama-3.1-8B     --dtype float16   --smooth false   --rotation true   --dataset wikitext2   --nsamples 128   --w_quantizer FixedQuantize   --w_group_size -1   --gptq_mse false   --gptq_groupsize -1   --save_path ./qqq-llama3-8b_base   --batch_size 8
# llama3.1 8b base per



# llama2 7b ÌÄÄÌä∏Ìä∏ 

# --gptq_groupsize=-1(per-channel) vs 128(per-group) Ï°∞Ìï©Ïóê Îî∞Îùº 
# ùêª
# H Î∏îÎ°ù ÌÅ¨Í∏∞ÏôÄ Íµ¨Ï°∞Í∞Ä Îã¨ÎùºÏßÄÍ≥†, Llama-2 ÌäπÏ†ï Í∑∏Î£π ÌòïÌÉúÍ∞Ä PD Ï°∞Í±¥ÏùÑ ÎßåÏ°±ÌïòÏßÄ Î™ªÌïòÍ≤å Îê† Ïàò ÏûàÏäµÎãàÎã§.

PYTHONPATH=. CUDA_VISIBLE_DEVICES=2,3,5 python3 examples/quant_model.py \
  --model_path     /home/eiclab/eiclab04/urp2025/_QQQ_origin/QQQ_jaeuk/Llama-2-7b \
  --tokenizer_path /home/eiclab/eiclab04/urp2025/_QQQ_origin/QQQ_jaeuk/Llama-2-7b \
  --dtype float16 \
  --smooth false \
  --rotation true \
  --dataset wikitext2 \
  --nsamples 128 \
  --w_quantizer FixedQuantize \
  --w_group_size -1 \
  --gptq_mse true \
  --gptq_groupsize -1 \
  --save_path ./qqq-llama2-7b_bolleanmask \
  --batch_size 8

# llama2 7b ÌÄÄÌä∏Ìä∏ mse False -> ÎèåÎ¶¨Í∏∞ÏúÑÌï¥
PYTHONPATH=. CUDA_VISIBLE_DEVICES=2,3,5 python3 examples/quant_model.py \
  --model_path     /home/eiclab/eiclab04/urp2025/_QQQ_origin/QQQ_jaeuk/Llama-2-7b \
  --tokenizer_path /home/eiclab/eiclab04/urp2025/_QQQ_origin/QQQ_jaeuk/Llama-2-7b \
  --dtype float16 \
  --smooth false \
  --rotation true \
  --dataset wikitext2 \
  --nsamples 128 \
  --w_quantizer FixedQuantize \
  --w_group_size -1 \
  --gptq_mse false \
  --gptq_groupsize -1 \
  --save_path ./qqq-llama2-7b_bolleanmask \
  --batch_size 8


# llama eval
# llama2 7b eval
CUDA_VISIBLE_DEVICES=3 PYTHONPATH=. python3 examples/eval_model.py   --model_path ./qqq-llama3-8b_test_more_samples/Llama-3.1-8B    --tokenizer_path ./qqq-llama3-8b_test_more_samples/Llama-3.1-8B   --eval_ppl   --batch_size 8   --max_length 2048

# llama2 7b eval
CUDA_VISIBLE_DEVICES=3 PYTHONPATH=. python3 examples/eval_model.py   --model_path ./qqq-llama2-7b_nopatch/Llama-2-7b    --tokenizer_path ./qqq-llama2-7b_nopatch/Llama-2-7b   --eval_ppl   --batch_size 8   --max_length 2048




# llama3.1 8b eval
CUDA_VISIBLE_DEVICES=1,2,3 PYTHONPATH=. python3 examples/eval_model.py   --model_path ./qqq-llama3-8b_flash/Llama-3.1-8B    --tokenizer_path ./qqq-llama3-8b_flash/Llama-3.1-8B   --eval_ppl   --batch_size 8   --max_length 2048


CUDA_VISIBLE_DEVICES=3 PYTHONPATH=. python3 examples/eval_model.py   --model_path ./qqq-llama3-8b_flash/Llama-3.1-8B    --tokenizer_path ./qqq-llama3-8b_flash/Llama-3.1-8B   --eval_ppl   --batch_size 1   --max_length 2048


qqq-llama3-8b_test_more_samples/

CUDA_VISIBLE_DEVICES=3 PYTHONPATH=. python3 examples/eval_model.py   --model_path ./qqq-llama3-8b_base/Llama-3.1-8B    --tokenizer_path ./qqq-llama3-8b_base/Llama-3.1-8B   --eval_ppl   --batch_size 1   --max_length 2048

CUDA_VISIBLE_DEVICES=3 PYTHONPATH=. python3 examples/eval_model.py   --model_path ./qqq-llama3-8b_g128/Llama-3.1-8B    --tokenizer_path ./qqq-llama3-8b_g128/Llama-3.1-8B   --eval_ppl   --batch_size 1   --max_length 2048