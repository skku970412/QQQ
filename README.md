# QQQ: QualityÂ Quattuorâ€‘BitÂ Quantization for Large LanguageÂ Models

> **Effortlessly compressâ€¯â€”â€¯and *accelerate*â€¯â€”â€¯LLMs with 4â€‘bit weights (W4) & 8â€‘bit activations (A8).**

QQQ is a researchâ€‘driven, hardwareâ€‘friendly W4A8 postâ€‘training quantization toolkit.
It allows **3.18â€¯Bâ€‘parameter and larger models** to run up to **â‰ˆâ€¯2.2Ã— faster** than their FP16 counterparts while retaining nearâ€‘original perplexity and zeroâ€‘shot accuracy.

<div align="center">
  <img src="assets/figures/throughput.png" alt="Throughput comparison" width="600"/>
</div>

---

## ğŸ”‘ Key Features

* **W4A8 endâ€‘toâ€‘end pipeline** â€“ adaptive activation smoothing + Hessianâ€‘guided weight compensation.
* **Custom CUDA GEMM kernels** â€“ perâ€‘channel & perâ€‘group W4A8 GEMMs deliver up to **3.7Ã—** speedâ€‘up over cuBLAS FP16.
* **Rotation & GPTQ hooks** â€“ optional weight rotation and MSEâ€‘optimised GPTQ blocks for extra accuracy.
* **Narrow model support** â€“
  *This fork adds turnâ€‘key scripts for a 3.18â€¯B custom model.*
* **vLLM integration** â€“ oneâ€‘line deployment on the highâ€‘throughput vLLM runtime.

---

## ğŸ—‚ï¸ Repository Layout

| Path                              | Whatâ€™s inside                                                                 |
| --------------------------------- | ----------------------------------------------------------------------------- |
| `assets/figures/`                 | Experiment plots & diagrams                                                   |
| `csrc/`                           | C++ / CUDA kernels and Triton utilities                                       |
| `examples/`                       | Python reference scripts (`quant_model.py`, `eval_model.py`, `test_model.py`) |
| `scripts/`                        | Handy shell wrappers for batch jobs (quantize / eval / infer)                 |
| `third-party/`                    | Vendored codeÂ â€” fast-hadamard-transformÂ â€¦                          |
| Topâ€‘levelÂ `*.ipynb`               | Reproducible notebooks for ablation & LLaMAâ€‘3 evaluation                      |
| `environment.yml`, `env_vars.txt` | Conda manifest & env variable template                                        |
| `setup.py` / `requirements.txt`   | PEPâ€‘517 build & minimal pip deps                                              |

ğŸ“„ *The full directory listing is visible on GitHub* ([github.com](https://github.com/skku970412/QQQ/tree/main))

---

## ğŸš€ Quick Installation

```bash
# clone & enter
git clone https://github.com/skku970412/QQQ.git
cd QQQ

# create the full Conda env (PythonÂ 3.9 / CUDAÂ 12.4)
conda env create -f environment.yml
conda activate qqq-py39

# build C++/CUDA extensions
pip install -v -e .
```

For alternative setups or lighter images, see **README\_conda.md**.

---

## âš¡ï¸ Quick Start

### 1. Quantise a model

```bash
python examples/quant_model.py \
  --model_path  /path/to/fp16-model \
  --tokenizer_path /path/to/tokenizer \
  --dtype float16 \
  --smooth false \   # enable SmoothQuant style smoothing if needed
  --rotation true \   # optional weight rotation
  --dataset wikitext2 --nsamples 128 \
  --w_quantizer FixedQuantize --w_group_size -1 \
  --gptq_mse true --gptq_groupsize -1 \
  --save_path  /path/to/w4a8-model
```

### 2. Evaluate perplexity & zeroâ€‘shot accuracy

```bash
python examples/eval_model.py \
  --model_path      /path/to/w4a8-model \
  --tokenizer_path  /path/to/tokenizer \
  --tasks "piqa,winogrande,hellaswag,arc_challenge,arc_easy" \
  --eval_ppl --batch_size 8 --max_length 2048
```

### 3. Inference with vLLM

```
pip install vllm
python vllm_serv.py
```

---

## ğŸ—“ï¸ Changelog (highlights)

* **2025â€‘03â€‘12**Â Â Paper accepted at **ICLRÂ 2025Â SCIâ€‘FM workshop**.([github.com](https://github.com/HandH1998/QQQ?utm_source=chatgpt.com))
* **2024â€‘09â€‘26**Â Â Smooth calibration code refactored; custom datasets supported.
* **2024â€‘09â€‘12**Â Â Added Qwenâ€‘2 models (0.5â€¯BÂ â†’Â 72â€¯B).
* **2024â€‘08â€‘26**Â Â Integrated weight rotation (accuracy â†‘, no latency cost).
* **2024â€‘07â€‘31**Â Â Merged into **vLLM** master; see linked PR for details.
* **2024â€‘07â€‘17**Â Â `quant_config.json` now autoâ€‘embedded in `config.json`.
* **2024â€‘06â€‘17**Â Â Preâ€‘print released on arXiv.
* **2024â€‘06â€‘03**Â Â Initial code release.

---

## ğŸ¤ Contributing

Pull requests are welcomeÂ â€” especially for **new model recipes, bug fixes, and kernel optimisations**.
Please create an issue first if you plan a large change.

---

## ğŸ“œ License & Citation

QQQ is released under the **ApacheÂ 2.0** license.
If you use this codebase or its kernels in your research, please cite:

```bibtex
@article{zhang2024qqq,
  title   = {QQQ: Quality Quattuor-Bit Quantization for Large Language Models},
  author  = {Ying Zhang and Peng Zhang and Mincong Huang and Jingyang Xiang and Yujie Wang and Chao Wang and Yineng Zhang and Lei Yu and Chuan Liu and Wei Lin},
  journal = {arXiv preprint arXiv:2406.09904},
  year    = 2024
}
```

---

*Happy quantising!* ğŸ‰
