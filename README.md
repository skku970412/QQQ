````markdown
## ğŸ“¦ QQQ: LLaMA 3.1â€“8B ì „ìš© W4A8 ì–‘ìí™” íˆ´í‚·

**QQQ**ëŠ” **LLaMA 3.1 8B ëª¨ë¸ ì „ìš©**ìœ¼ë¡œ ì„¤ê³„ëœ ì—°êµ¬ ê¸°ë°˜ì˜ W4A8(4ë¹„íŠ¸ ê°€ì¤‘ì¹˜, 8ë¹„íŠ¸ í™œì„±í™”) í¬ìŠ¤íŠ¸ íŠ¸ë ˆì´ë‹ ì–‘ìí™” íˆ´í‚·ì…ë‹ˆë‹¤.  
ë‹¤ë¥¸ ëª¨ë¸ì€ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©°, **ì˜¤ì§ LLaMA 3.1â€“8B**ë§Œì„ ìœ„í•´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

### ğŸ”‘ ì£¼ìš” ê¸°ëŠ¥ (LLaMA 3.1â€“8B ì „ìš©)

- **W4A8 ì „ì²´ ì–‘ìí™” íŒŒì´í”„ë¼ì¸**  
  - SmoothQuant ìŠ¤íƒ€ì¼ í™œì„±í™” í‰í™œí™” (ì„ íƒì )  
  - í—¤ì‹œì•ˆ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ë³´ì •  
- **ì»¤ìŠ¤í…€ CUDA GEMM ì»¤ë„**  
  - per-channel ë° per-group W4A8 ì—°ì‚°  
  - cuBLAS FP16 ëŒ€ë¹„ ìµœëŒ€ **3.7Ã—** ì†ë„ í–¥ìƒ  
- **ì •í™•ë„ í–¥ìƒ ì˜µì…˜**  
  - ì„ íƒì  Weight Rotation  
  - MSE ìµœì í™” GPTQ ë¸”ë¡  
- **vLLM í†µí•© ì§€ì›**  
  - `--quantization qqq` ì˜µì…˜ìœ¼ë¡œ Marlin ì»¤ë„ ê¸°ë°˜ ê³ ì† ì¶”ë¡ 

---

### âœ… ì§€ì› ëª¨ë¸

| ëª¨ë¸                | ì§€ì› ì—¬ë¶€ |
|---------------------|-----------|
| LLaMA 3.1 8B        | âœ… ì§€ì›   |
| ê¸°íƒ€ ëª¨ë“  ëª¨ë¸      | âŒ ë¯¸ì§€ì› |

> ğŸ“Œ ë³¸ ì €ì¥ì†ŒëŠ” **LLaMA 3.1 8B ì „ìš©**ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì—ëŠ” ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

---

### ğŸš€ ë¹ ë¥¸ ì„¤ì¹˜

```bash
git clone https://github.com/skku970412/QQQ.git
cd QQQ
conda env create -f environment.yml
conda activate qqq-py39
pip install -v -e .
````

---

### âš¡ï¸ ë¹ ë¥¸ ì‹œì‘

1. **ì–‘ìí™”**

   ```bash
   python examples/quant_model.py \
     --model_path /path/to/llama3.1-8b-fp16 \
     --tokenizer_path /path/to/tokenizer \
     --save_path /path/to/llama3.1-8b-w4a8 \
     --dataset wikitext2 --nsamples 128 \
     --w_quantizer FixedQuantize \
     --gptq_mse true --rotation true
   ```
2. **ì„±ëŠ¥ í‰ê°€** (Perplexity ë° Zero-Shot Accuracy)

   ```bash
   python examples/eval_model.py \
     --model_path /path/to/llama3.1-8b-w4a8 \
     --tokenizer_path /path/to/tokenizer \
     --tasks "piqa,winogrande,hellaswag,arc_challenge,arc_easy" \
     --eval_ppl --batch_size 8 --max_length 2048
   ```
3. **vLLM ì¶”ë¡ **

   ```bash
   pip install vllm
   python vllm_serv.py
   ```

---

### ğŸ“ ë³€ê²½ ì´ë ¥ (í•˜ì´ë¼ì´íŠ¸)(ë‚´ê°€í•œê±°ì•„ë‹˜._.)

* **2025-03-12** ICLR 2025 SCI-FM ì›Œí¬ìˆ ì±„íƒ
* **2024-09-26** SmoothQuant ë¦¬íŒ©í† ë§ ë° ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì§€ì›
* **2024-09-12** Qwen-2 ëª¨ë¸(0.5 B â†’ 72 B) ì¶”ê°€
* **2024-08-26** Weight Rotation í†µí•© (ì§€ì—° ì‹œê°„ ì¦ê°€ ì—†ìŒ)
* **2024-07-31** vLLM ê³µì‹ ë¨¸ì§€ ì™„ë£Œ (PR ì°¸ì¡°)
* **2024-07-17** `quant_config.json` ìë™ ë‚´ì¥
* **2024-06-17** arXiv ì‚¬ì „ ì¸ì‡„ ë¦´ë¦¬ìŠ¤
* **2024-06-03** ì´ˆê¸° ì½”ë“œ ê³µê°œ
* ì™€... ã„·ã„· ë ˆì „ë“œ

---


ğŸ“œ **ë¼ì´ì„ ìŠ¤ ë° ì¸ìš©**
Apache 2.0 â€” ì—°êµ¬ì— ì‚¬ìš©í•˜ì‹¤ ê²½ìš° ì•„ë˜ ë…¼ë¬¸ì„ ì¸ìš©í•´ ì£¼ì„¸ìš”:

```bibtex
@article{zhang2024qqq,
  title   = {QQQ: Quality Quattuor-Bit Quantization for Large Language Models},
  author  = {Ying Zhang and Peng Zhang and Mincong Huang and Jingyang Xiang and Yujie Wang and Chao Wang and Yineng Zhang and Lei Yu and Chuan Liu and Wei Lin},
  journal = {arXiv preprint arXiv:2406.09904},
  year    = 2024
}
```

```
```
